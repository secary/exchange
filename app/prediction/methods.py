import sys
import os

# è‡ªåŠ¨åŠ å…¥é¡¹ç›®æ ¹ç›®å½•åˆ° sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")))

from loguru import logger
import uuid
from config.logger_config import trace_ids

# âœ… ç»‘å®š loguru çš„ name å­—æ®µï¼Œç”¨äºæ—¥å¿—åˆ†ç±»è¾“å‡º
logger = logger.bind(name="jervis")
# è®¾ç½® trace_idï¼ˆç‹¬ç«‹è¿è¡Œæ—¶ä½¿ç”¨ uuidï¼›ä¹Ÿæ”¯æŒä»ç¯å¢ƒå˜é‡ä¼ å…¥ï¼‰
trace_id = os.getenv("TRACE_ID_JERVIS") or f"JERVIS-{uuid.uuid4()}"
trace_ids["jervis"].set(trace_id)

import pandas as pd
import numpy as np
from sqlalchemy.orm import sessionmaker
from datetime import datetime, timedelta
from config.settings import get_engine
from app.models import History

from sklearn.preprocessing import MinMaxScaler
import torch
from app.prediction.models.lstm import RateLSTM  # âœ… ä¿æŒç»å¯¹è·¯å¾„

scaler = MinMaxScaler()

def fetch_history(currency: str | None=None, days: int | None=None) -> pd.DataFrame:
    """
    è¿”å›æœ€è¿‘ 30 å¤©çš„ History è®°å½•ï¼Œç»“æœä¸º Pandas DataFrameã€‚
    """
    Session = sessionmaker(bind=get_engine())
    session = Session()
    currency = currency.upper()
    
    try:
        query = session.query(History)
        
        if days:
            start_time = datetime.now() - timedelta(days=days)
            query = session.query(History).filter(History.Date >= start_time)

        if currency:
            query = query.filter(History.Currency == currency)

        # å¾—åˆ° ORM å¯¹è±¡åˆ—è¡¨
        rows = query.order_by(History.Date.asc()).all()

        # è½¬æˆå­—å…¸åˆ—è¡¨
        data = [
            {
                "Date": r.Date,
                "Currency": r.Currency,
                "Rate": r.Rate,
                "Locals": r.Locals,
            }
            for r in rows
        ]

        return pd.DataFrame(data)
    finally:
        session.close()
        

def batchify(X, y, batch_size):
    for i in range(0, len(X), batch_size):
        yield X[i:i+batch_size], y[i:i+batch_size]

def scale(x, scaler=scaler, inverse: bool=False):
    if not inverse:
        return scaler.fit_transform(x)
    
    return scaler.inverse_transform(x)

     
def build_sequences(series, seq_len, verbose: bool = False):
    X, y = [], []
    for i in range(len(series) - seq_len):
        X.append(series[i : i + seq_len])
        y.append(series[i + seq_len])
    X, y = np.array(X), np.array(y)
    
    if verbose:
        print(f"Tensor shape: {X.shape}")
    return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)


def split(X, y, train_ratio: float, verbose: bool = False) -> tuple:
    TRAIN_SIZE = int(len(X) * train_ratio)
    X_train, y_train = X[:TRAIN_SIZE], y[:TRAIN_SIZE]
    X_test,  y_test  = X[TRAIN_SIZE:], y[TRAIN_SIZE:]
    
    if verbose:
        print(f"Train size: {X_train.shape[0]}\nTest size: {X_test.shape[0]}")
    return X_train, y_train, X_test, y_test
    
def load_latest_model(model_dir: str, currency: str, device: str = "cpu") -> RateLSTM:
    """
    ä»æŒ‡å®šç›®å½•ä¸­åŠ è½½æœ€æ–°çš„ RateLSTM æ¨¡å‹ï¼ˆ.pth æ–‡ä»¶ï¼‰ã€‚
    å¦‚æœæ‰¾ä¸åˆ°æ¨¡å‹ï¼Œå°†è‡ªåŠ¨è°ƒç”¨è®­ç»ƒå‡½æ•°ã€‚
    """
    currency = currency.upper()
    os.makedirs(model_dir, exist_ok=True)
    
    def find_latest_file():
        files = [
            f for f in os.listdir(model_dir)
            if f.endswith(".pth") and f"RateLSTM_{currency}_" in f
        ]
        files.sort()
        return files[-1] if files else None

    # å°è¯•ç¬¬ä¸€æ¬¡æŸ¥æ‰¾æ¨¡å‹
    latest_file = find_latest_file()

    if not latest_file:
        logger.error(f"âš ï¸ æœªæ‰¾åˆ° {currency} æ¨¡å‹ï¼Œå°è¯•è‡ªåŠ¨è®­ç»ƒ...")
        import app.prediction.tune_lstm
        app.prediction.tune_lstm.main(currency)  # è‡ªåŠ¨è®­ç»ƒ
        latest_file = find_latest_file()

        if not latest_file:
            logger.error(f"âŒ è‡ªåŠ¨è®­ç»ƒåä»æœªæ‰¾åˆ°æ¨¡å‹: {currency}")

    latest_path = os.path.join(model_dir, latest_file)
    logger.info(f"ğŸ” Loading latest {currency} model: {latest_path}")

    model = RateLSTM().to(device)
    model.load_state_dict(torch.load(latest_path, map_location=device))
    model.eval()
    return model
    
from sklearn.metrics import mean_absolute_error, mean_squared_error

def evaluate_metrics(y_true, y_pred, verbose: bool = True) -> dict:
    """
    è¯„ä¼°é¢„æµ‹ç»“æœçš„å¸¸ç”¨å›å½’æŒ‡æ ‡ã€‚

    å‚æ•°:
        y_true: çœŸå®å€¼ (1D array æˆ– Tensor)
        y_pred: é¢„æµ‹å€¼ (1D array æˆ– Tensor)
        verbose: æ˜¯å¦æ‰“å°æŒ‡æ ‡

    è¿”å›:
        ä¸€ä¸ªåŒ…å« MAE, MSE, RMSE, MAPE çš„å­—å…¸
    """
    # è‹¥ä¸º tensorï¼Œè½¬ä¸º numpy
    if hasattr(y_true, 'detach'):
        y_true = y_true.detach().cpu().numpy()
    if hasattr(y_pred, 'detach'):
        y_pred = y_pred.detach().cpu().numpy()

    y_true = np.array(y_true).flatten()
    y_pred = np.array(y_pred).flatten()

    mae = mean_absolute_error(y_true, y_pred)
    mse = mean_squared_error(y_true, y_pred)
    rmse = np.sqrt(mse)

    # é¿å…é™¤ä»¥ 0
    nonzero_mask = y_true != 0
    if np.any(nonzero_mask):
        mape = np.mean(np.abs((y_true[nonzero_mask] - y_pred[nonzero_mask]) / y_true[nonzero_mask])) * 100
    else:
        mape = np.nan

    if verbose:
        print(f"MAE  : {mae:.8f}")
        print(f"MSE  : {mse:.8f}")
        print(f"RMSE : {rmse:.8f}")
        print(f"MAPE : {mape:.2f}%")

    return {
        "mae": mae,
        "mse": mse,
        "rmse": rmse,
        "mape": mape
    }
    
def preprocess(data: pd.DataFrame) -> pd.DataFrame:
    data = data.copy()[["Date", 'Rate']] 
    data["Date"] = pd.to_datetime(data["Date"])
    data = data.set_index("Date").sort_index()
    data = data.resample("0.5h").mean().interpolate()
    return data